---
title: "Metrics and ML - Original Imputation Method"
author: "Sydney Gu and Isabella Lin"
date: "2025-02-02"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(haven)
library(glmnet)
library(data.table)
library(tensorflow)
library(keras)
```
To use the keras package, we need to make sure that TensorFlow is installed and can be used correctly:
```{r}
library(reticulate)
use_condaenv("r-tensorflow", required = TRUE)

library(tensorflow)

# Print TensorFlow configuration properly
# tf_info <- tensorflow::tf_config()
# str(tf_info)  # View the structure of the config output

# Manually extract the TensorFlow version
# tf_version <- tf_info$version
# cat("TensorFlow Version:", tf_version, "\n")

# Manually extract the Python binary path
# py_path <- tf_info$python
# cat("Using Python at:", py_path, "\n")

```


Run the cleaning.Rmd file first, in order to clean the CEX and PSID datasets.

Now, we're going to train our neural network to predict non-durable consumption.
```{r}
# feature columns (remove Y)
feature_cols <- setdiff(names(cex), "ndur")

# convert data to matrices for keras
X_data <- as.matrix(cex[, feature_cols])  # Features
Y_data <- as.matrix(cex$ndur) 

# split CEX data into training and testing sets (80/20 split)
set.seed(42)
train_idx <- sample(1:nrow(cex), size = 0.8 * nrow(cex))

X_train <- X_data[train_idx, ]
Y_train <- Y_data[train_idx]
X_test <- X_data[-train_idx, ]
Y_test <- Y_data[-train_idx]
```

Define our model:
```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(X_train)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear") 
```

debugging:
```{r}
sum(is.na(X_train))  # Check for missing values in features
sum(is.nan(X_train))  # Check for NaNs
sum(is.infinite(X_train))  # Check for infinite values

sum(is.na(Y_train))  # Check for missing values in target
sum(is.nan(Y_train))  # Check for NaNs
sum(is.infinite(Y_train))  # Check for infinite values
```
let's adjust all the NA values in X_train to equal 0:
```{r}
X_train[is.na(X_train)] <- 0
```

Normalize the values in the training set:
```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x) + 1e-8))  # Adding small value to prevent division by zero
}
X_train <- apply(X_train, 2, normalize)  # Apply function to columns
```

convert both back to matrices just to be safe:
```{r}
Y_train <- matrix(Y_train, ncol = 1)
```


Compile it:
```{r}
model %>% compile(
  loss = "mse",  # Mean Squared Error (MSE) for regression
  optimizer = optimizer_adam(lr = 0.001),  # Adam optimizer
  metrics = c("mae")  # Mean Absolute Error (MAE) for evaluation
)
```

Train it:
```{r}
history <- model %>% fit(
  X_train, Y_train,
  epochs = 50,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 1
)
```

some more quick debugging:
```{r}
sum(is.na(X_test))  # Should be 0
sum(is.nan(X_test))  # Should be 0
sum(is.infinite(X_test))  # Should be 0

sum(is.na(Y_test))  # Should be 0
sum(is.nan(Y_test))  # Should be 0
sum(is.infinite(Y_test))  # Should be 0
```
```{r}
X_test[is.na(X_test)] <- 0
```


```{r}
# Evaluate on test set
model %>% evaluate(X_test, Y_test)
```

```{r}
# Predict consumption values (ndur) for test data
predictions <- model %>% predict(X_test)
```


