---
title: "Metrics and ML - Imputation with Gradient Boosting Machines"
author: "Sydney Gu and Isabella Lin"
date: "2025-02-02"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(haven)
library(glmnet)
library(data.table)
library(tensorflow)
library(keras)
library(caret)
library(gbm)
```


Let's prepare our data:
```{r}
# Ensure the data is in the correct format
cex$ndur <- as.numeric(cex$ndur)  # Convert target to numeric if needed

# Split data into training and testing sets
set.seed(123)  # For reproducibility
trainIndex_GBM <- createDataPartition(cex$ndur, p = 0.8, list = FALSE)
trainGBM <- cex[trainIndex_GBM, ]
testGBM <- cex[-trainIndex_GBM, ]
```

Now, let's train our GBM:
```{r}
# Train the GBM model
set.seed(123)
gbm_model <- gbm(
  formula = ndur ~ .,         # Predict ndur using all other features
  data = trainGBM,
  distribution = "gaussian",    # Use Gaussian for regression
  n.trees = 1000,               # Number of boosting iterations
  interaction.depth = 3,        # Depth of each tree
  shrinkage = 0.01,             # Learning rate
  n.minobsinnode = 10,          # Minimum observations per tree node
  cv.folds = 5,                 # Cross-validation folds
  verbose = FALSE               # Set to TRUE for progress updates
)

```

Let's evaluate the performance of our trained GBM:
```{r}
# Check best number of trees using cross-validation
best_trees <- gbm.perf(gbm_model, method = "cv")

# Make predictions
predictions <- predict(gbm_model, newdata = testGBM, n.trees = best_trees)

# Calculate RMSE (Root Mean Squared Error)
rmse <- sqrt(mean((testGBM$ndur - predictions)^2))
print(paste("RMSE:", rmse))

# Plot feature importance
summary(gbm_model)
```
```{r}
length(testGBM$ndur) == length(predictions)  # Should return TRUE
```

```{r}
# Compute MAE
gbm_mae <- mean(abs(testGBM$ndur - predictions))

# Print MAE result
print(paste("Test MAE:", gbm_mae))
```




Let's also fit a GBM model with crossvalidation:
```{r}
# check for missing data
colSums(is.na(trainGBM))
```
```{r}
trainGBM[is.na(trainGBM)] <- 0
```


```{r}
# Define training control with 5-fold CV
train_control <- trainControl(
  method = "cv",         # k-fold cross-validation
  number = 5,            # Number of folds
  verboseIter = TRUE     # Show training progress
)
```

```{r}
# Train GBM with CV
set.seed(123)
gbm_cv <- train(
  ndur ~ .,             # Formula: predict `nondur` using all features
  data = trainGBM,
  method = "gbm",         # Use Gradient Boosting
  trControl = train_control,  # Apply cross-validation
  verbose = FALSE         # Suppress printing details
)

# Print results
print(gbm_cv)
```
Now, let's evaluate the model performance:
```{r}
# View best hyperparameters
print(gbm_cv$bestTune)

# Make predictions
gbm_cv_pred <- predict(gbm_cv, newdata = testGBM)

# Calculate RMSE (Root Mean Squared Error)
gbm_cv_rmse <- sqrt(mean((testGBM$ndur - gbm_cv_pred)^2))
print(paste("Test RMSE:", gbm_cv_rmse))
```
```{r}
length(testGBM$ndur) == length(gbm_cv_pred)  # Should return TRUE
```



