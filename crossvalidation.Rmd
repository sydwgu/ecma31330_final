---
title: "Metrics and ML - Crossvalidation"
author: "Sydney Gu and Isabella Lin"
date: "2025-02-02"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(haven)
library(glmnet)
library(data.table)
library(tensorflow)
library(keras)
library(caret)
```

To use the keras package, we need to make sure that TensorFlow is installed and can be used correctly:
```{r}
library(reticulate)
use_condaenv("r-tensorflow", required = TRUE)

library(tensorflow)

# Print TensorFlow configuration properly
# tf_info <- tensorflow::tf_config()
# str(tf_info)  # View the structure of the config output

# Manually extract the TensorFlow version
# tf_version <- tf_info$version
# cat("TensorFlow Version:", tf_version, "\n")

# Manually extract the Python binary path
# py_path <- tf_info$python
# cat("Using Python at:", py_path, "\n")

```


Run the cleaning.Rmd file first, in order to clean the CEX and PSID datasets.

Now, we're going to train our neural network to predict non-durable consumption.
```{r}
# feature columns (remove Y)
feature_cols <- setdiff(names(cex), "ndur")

# convert data to matrices for keras
X_data <- as.matrix(cex[, feature_cols])  # Features
Y_data <- as.matrix(cex$ndur) 

# split CEX data into training and testing sets (80/20 split)
set.seed(42)
train_idx <- sample(1:nrow(cex), size = 0.8 * nrow(cex))

X_train <- X_data[train_idx, ]
Y_train <- Y_data[train_idx]
X_test <- X_data[-train_idx, ]
Y_test <- Y_data[-train_idx]
```

```{r}
X_train[is.na(X_train)] <- 0
```

Normalize the values in the training set:
```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x) + 1e-8))  # Adding small value to prevent division by zero
}
X_train <- apply(X_train, 2, normalize)  # Apply function to columns
```

convert both back to matrices just to be safe:
```{r}
Y_train <- matrix(Y_train, ncol = 1)
```


```{r}
X_test[is.na(X_test)] <- 0
```
```{r}
X_test <- apply(X_test, 2, normalize)  # Ensure the same scaling as training
X_test <- as.matrix(X_test)
```

reshape Y_test:
```{r}
Y_test <- matrix(Y_test, ncol = 1)
```


```{r}
library(caret)
folds <- createFolds(Y_train, k = 10)

cv_results <- list()

for (i in seq_along(folds)) {
  train_idx <- unlist(folds[-i])  # Training indices
  test_idx <- folds[[i]]  # Validation indices

  X_train_cv <- X_train[train_idx, ]
  Y_train_cv <- Y_train[train_idx]
  X_val_cv <- X_train[test_idx, ]
  Y_val_cv <- Y_train[test_idx]

  # Define model
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu", input_shape = ncol(X_train_cv)) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 16, activation = "relu") %>%
    layer_dense(units = 1, activation = "linear")

  # Compile
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_adam(learning_rate = 0.001),
    metrics = c("mae")
  )

  # Train
  history <- model %>% fit(
    X_train_cv, Y_train_cv,
    epochs = 50,
    batch_size = 32,
    validation_data = list(X_val_cv, Y_val_cv),
    verbose = 1
  )

  # Evaluate
  eval_results <- model %>% evaluate(X_val_cv, Y_val_cv)
  cv_results[[i]] <- eval_results
}

```

calculate MAE across folds:
```{r}
# Calculate average MAE across folds
mean(unlist(lapply(cv_results, function(x) x["mae"])))
```


calculate average loss (MSE) across folds:
```{r}
mean(unlist(lapply(cv_results, function(x) x["loss"])))
```

```{r}
sqrt(mean(unlist(lapply(cv_results, function(x) x["loss"]))))
```

